{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "367fbb32",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron & Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d0b95",
   "metadata": {},
   "source": [
    "This notebook presents a novel architecture of multilayer perceptron and autoencoder inspired by a [top-1 solution](https://www.kaggle.com/c/jane-street-market-prediction/discussion/224348) for Jane Street market competition. Both components of the system are training together on the same data. The latent-space representation produced by Autoencoder is concatenated with original set of attributes and fed into MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e288403f",
   "metadata": {},
   "source": [
    "### Backlog:\n",
    "1. <s> Concatenate output of AE with the input (use latent representation for a new features) </s>\n",
    "2. <s> Try Swish (SiLU) activation function </s>\n",
    "3. Add Gaussian noise layer before encoder for data augmentation\n",
    "4. Add target information to autoencoder (supervised learning) to force it to generate more relevant features, and to create a shortcut for backpropagation of gradient\n",
    "5. Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa7881",
   "metadata": {},
   "source": [
    "### 0. Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edfa6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import torch\n",
    "import optuna\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import OrderedDict\n",
    "\n",
    "from src.metrics import pearson_metric\n",
    "from src.torch_models.mlp import MLPAE\n",
    "from src.data import Dataset, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c348e122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371536eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "EPOCHS = 30\n",
    "EXPERIMENT = \"MLP+AE-baseline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2fd25b",
   "metadata": {},
   "source": [
    "### 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b28b5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading took 9.84 seconds\n"
     ]
    }
   ],
   "source": [
    "trainloader, _ = load_data(use_feather=True, split_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e05923",
   "metadata": {},
   "source": [
    "### 2. Building a Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74673da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPAE(input_dim=301, mlp_depth=3, activation=nn.SiLU).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "109f3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, loader, optimizer, investment_id_dropout=0.01, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    accuracy = []\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x[:, 0] *= (torch.rand(len(x)) > investment_id_dropout)\n",
    "        optimizer.zero_grad()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x_pred, y_pred = model(x)\n",
    "        \n",
    "        loss_ae = criterion(x, x_pred)\n",
    "        loss_mlp = criterion(y, y_pred.view(-1))\n",
    "        \n",
    "        loss = loss_ae + loss_mlp        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_ae = loss_ae.item()\n",
    "        loss_mlp = loss_mlp.item()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    losses = {\n",
    "        'ae': loss_ae / len(loader),\n",
    "        'mlp': loss_mlp / len(loader),\n",
    "        'ov': train_loss / len(loader)\n",
    "    }\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4818087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, criterion, loader, investment_id_dropout=0.01, device='cpu'):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    accuracy = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            x[:, 0] *= (torch.rand(len(x)) > investment_id_dropout)\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x_pred, y_pred = model(x)\n",
    "\n",
    "            loss_ae = criterion(x, x_pred)\n",
    "            loss_mlp = criterion(y, y_pred.view(-1))\n",
    "\n",
    "            loss = loss_ae + loss_mlp  \n",
    "            \n",
    "            loss_ae = loss_ae.item()\n",
    "            loss_mlp = loss_mlp.item()\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    losses = {\n",
    "        'ae': loss_ae / len(loader),\n",
    "        'mlp': loss_mlp / len(loader),\n",
    "        'ov': test_loss / len(loader)\n",
    "    }\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87965160",
   "metadata": {},
   "source": [
    "### 3. Training the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4933f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = os.path.join(\"weights\", EXPERIMENT)\n",
    "if not os.path.exists(experiment_dir):\n",
    "    os.makedirs(experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9779f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    start_execution = time.time()\n",
    "    train_losses = train(model, criterion, trainloader, optimizer, device=DEVICE)\n",
    "    # test_losses = test(model, criterion, testloader, device=DEVICE)    \n",
    "    scheduler.step()\n",
    "    # Test AE: {test_losses['ae']:.5f} MLP: {test_losses['mlp']:.5f} OV: {test_losses['ov']:.5f} |     \n",
    "    print(f\"Epoch: {epoch+1:02d} ({time.time()-start_execution:.1f} s.) | Train AE: {train_losses['ae']:.5f} MLP: {train_losses['mlp']:.5f} OV: {train_losses['ov']:.5f} |\")\n",
    "    \n",
    "    losses.append(train_losses['ov'])\n",
    "    if train_losses['ov'] <= min(losses):\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': train_losses['ov'], \n",
    "        }, os.path.join(experiment_dir, f\"{epoch}.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
